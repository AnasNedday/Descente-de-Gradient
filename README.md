# Descente-de-Gradient 
La Descente de Gradient est un algorithme d’optimisation qui permet de trouver le minimum de n’importe quelle fonction convexe en convergeant progressivement vers celui-ci.
